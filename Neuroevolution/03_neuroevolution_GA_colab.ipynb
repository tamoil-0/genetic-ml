{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 03 \u2014 Neuroevoluci\u00f3n con GA (Colab)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# (Colab) Instala/actualiza dependencias m\u00ednimas\n# Nota: Colab ya trae TensorFlow; si quieres fijar versi\u00f3n, descomenta la l\u00ednea de tensorflow\n!pip -q install --upgrade numpy pandas scikit-learn matplotlib joblib\n# !pip -q install --upgrade tensorflow>=2.12\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Uso\n1) Ejecuta la celda de instalaci\u00f3n (arriba).  \n2) Ejecuta la celda de **c\u00f3digo** para definir la clase `GeneticNeuroevolution`.  \n3) Ejecuta **RUN** para lanzar la evoluci\u00f3n y generar `evolution_progress.png`.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Configurar semillas para reproducibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Generar dataset de ejemplo\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=8, \n                          n_redundant=5, random_state=42)\nX = StandardScaler().fit_transform(X)\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclass GeneticNeuroevolution:\n    def __init__(self, n_population=10, n_generations=5, crossover_rate=0.6, \n                 mutation_rate=0.2, elitism=0.2):\n        # Par\u00e1metros del algoritmo gen\u00e9tico (reducidos para demo)\n        self.n_population = n_population\n        self.n_generations = n_generations\n        self.crossover_rate = crossover_rate\n        self.mutation_rate = mutation_rate\n        self.elitism = elitism\n        self.input_dim = X.shape[1]\n        \n        # Historial para gr\u00e1ficos\n        self.best_fitness_history = []\n        self.avg_fitness_history = []\n        \n    def initialize_population(self):\n        \"\"\"Inicializa la poblaci\u00f3n con arquitecturas aleatorias\"\"\"\n        population = []\n        for _ in range(self.n_population):\n            # Representaci\u00f3n del cromosoma: \n            # [n_layers, units_layer1, units_layer2, dropout_rate, learning_rate_log]\n            n_layers = np.random.randint(1, 3)  # 1-2 capas ocultas (simplificado)\n            individual = [n_layers]\n            \n            # Unidades por capa (simplificado)\n            for _ in range(n_layers):\n                individual.append(np.random.choice([32, 64, 128]))\n            \n            # Dropout rate (0.0 - 0.5)\n            individual.append(np.random.uniform(0.0, 0.5))\n            \n            # Learning rate (en escala logar\u00edtmica: 1e-4 to 1e-2)\n            individual.append(np.random.uniform(-4, -2))\n            \n            population.append(individual)\n        return population\n    \n    def build_model(self, individual):\n        \"\"\"Construye un modelo de Keras basado en el individuo\"\"\"\n        try:\n            n_layers = individual[0]\n            units = individual[1:1+n_layers]\n            dropout_rate = individual[1+n_layers]\n            learning_rate = 10 ** individual[2+n_layers]  # Convertir de log scale\n            \n            # Validar valores\n            if dropout_rate < 0 or dropout_rate > 1:\n                return None\n            if learning_rate <= 0:\n                return None\n            \n            model = Sequential()\n            model.add(Dense(units[0], activation='relu', input_shape=(self.input_dim,)))\n            model.add(Dropout(dropout_rate))\n            \n            for u in units[1:]:\n                model.add(Dense(u, activation='relu'))\n                model.add(Dropout(dropout_rate))\n            \n            model.add(Dense(1, activation='sigmoid'))\n            \n            model.compile(\n                optimizer=Adam(learning_rate=learning_rate),\n                loss='binary_crossentropy',\n                metrics=['accuracy']\n            )\n            return model\n        except:\n            return None\n    \n    def fitness_function(self, individual):\n        \"\"\"Eval\u00faa la aptitud de una arquitectura de red neuronal\"\"\"\n        model = self.build_model(individual)\n        if model is None:\n            return -1.0  # Penalizar individuos inv\u00e1lidos\n        \n        try:\n            # Entrenamiento r\u00e1pido para evaluaci\u00f3n\n            history = model.fit(\n                X_train, y_train,\n                epochs=5,  # Reducido para mayor velocidad\n                batch_size=32,\n                validation_split=0.2,\n                verbose=0\n            )\n            \n            # Fitness basado en accuracy de validaci\u00f3n\n            val_accuracy = max(history.history['val_accuracy'])\n            \n            # Penalizar arquitecturas complejas\n            complexity_penalty = 0.001 * sum(individual[1:1+individual[0]]) / 100\n            \n            return val_accuracy - complexity_penalty\n            \n        except Exception as e:\n            print(f\"Error en entrenamiento: {e}\")\n            return -1.0\n    \n    def selection(self, population, fitness_scores):\n        \"\"\"Selecci\u00f3n por m\u00e9todo de ruleta\"\"\"\n        # Reemplazar valores negativos con 0\n        fitness_scores = np.array([max(0, f) for f in fitness_scores])\n        \n        if np.sum(fitness_scores) == 0:\n            probabilities = np.ones(len(population)) / len(population)\n        else:\n            probabilities = fitness_scores / np.sum(fitness_scores)\n        \n        selected_indices = np.random.choice(\n            len(population), \n            size=self.n_population, \n            p=probabilities,\n            replace=True\n        )\n        return [population[i] for i in selected_indices]\n    \n    def crossover(self, parent1, parent2):\n        \"\"\"Operador de cruzamiento de un punto\"\"\"\n        child1 = parent1.copy()\n        child2 = parent2.copy()\n        \n        if np.random.random() < self.crossover_rate:\n            # Cruzar n\u00famero de capas\n            if np.random.random() < 0.5:\n                child1[0], child2[0] = child2[0], parent1[0]\n            \n            # Ajustar longitud de hijos seg\u00fan n\u00famero de capas\n            for child in [child1, child2]:\n                n_layers = child[0]\n                expected_length = 1 + n_layers + 2  # n_layers + units + dropout + lr\n                if len(child) > expected_length:\n                    # Eliminar elementos extras\n                    child = child[:expected_length]\n                elif len(child) < expected_length:\n                    # A\u00f1adir elementos faltantes\n                    while len(child) < expected_length:\n                        if len(child) < 1 + n_layers:\n                            child.append(np.random.choice([32, 64, 128]))\n                        elif len(child) == 1 + n_layers:\n                            child.append(np.random.uniform(0.0, 0.5))\n                        else:\n                            child.append(np.random.uniform(-4, -2))\n            \n        return child1, child2\n    \n    def mutation(self, individual):\n        \"\"\"Operador de mutaci\u00f3n\"\"\"\n        individual = individual.copy()\n        \n        # Mutaci\u00f3n del n\u00famero de capas\n        if np.random.random() < self.mutation_rate:\n            new_n_layers = np.random.randint(1, 3)\n            if new_n_layers != individual[0]:\n                individual[0] = new_n_layers\n                # Ajustar unidades\n                current_units = individual[1:1+new_n_layers]\n                if len(current_units) < new_n_layers:\n                    # A\u00f1adir capas si es necesario\n                    for _ in range(new_n_layers - len(current_units)):\n                        individual.insert(1, np.random.choice([32, 64, 128]))\n                else:\n                    # Remover capas si es necesario\n                    individual = individual[:1+new_n_layers] + individual[1+len(current_units):]\n        \n        # Mutaci\u00f3n de unidades en capas\n        for i in range(1, 1 + individual[0]):\n            if np.random.random() < self.mutation_rate/2:\n                individual[i] = np.random.choice([32, 64, 128])\n        \n        # Asegurar que tenemos todos los par\u00e1metros necesarios\n        expected_length = 1 + individual[0] + 2\n        if len(individual) < expected_length:\n            # A\u00f1adir par\u00e1metros faltantes\n            while len(individual) < expected_length:\n                if len(individual) == 1 + individual[0]:\n                    individual.append(np.random.uniform(0.0, 0.5))\n                else:\n                    individual.append(np.random.uniform(-4, -2))\n        \n        # Mutaci\u00f3n de dropout rate\n        dropout_idx = 1 + individual[0]\n        if np.random.random() < self.mutation_rate/2:\n            individual[dropout_idx] = np.random.uniform(0.0, 0.5)\n        \n        # Mutaci\u00f3n de learning rate\n        lr_idx = 2 + individual[0]\n        if np.random.random() < self.mutation_rate/2:\n            individual[lr_idx] = np.random.uniform(-4, -2)\n        \n        return individual\n    \n    def evolve(self):\n        \"\"\"Ejecuta el proceso evolutivo\"\"\"\n        population = self.initialize_population()\n        best_individual_history = []\n        \n        print(\"Iniciando proceso de neuroevoluci\u00f3n...\")\n        \n        for generation in range(self.n_generations):\n            # Evaluar fitness de toda la poblaci\u00f3n\n            fitness_scores = []\n            for individual in population:\n                fitness = self.fitness_function(individual)\n                fitness_scores.append(fitness)\n            \n            # Guardar estad\u00edsticas\n            best_idx = np.argmax(fitness_scores)\n            best_fitness = fitness_scores[best_idx]\n            avg_fitness = np.mean(fitness_scores)\n            \n            self.best_fitness_history.append(best_fitness)\n            self.avg_fitness_history.append(avg_fitness)\n            best_individual_history.append(population[best_idx])\n            \n            print(f\"Generaci\u00f3n {generation + 1}: Mejor fitness = {best_fitness:.4f}, Fitness promedio = {avg_fitness:.4f}\")\n            \n            # Selecci\u00f3n\n            selected_population = self.selection(population, fitness_scores)\n            \n            # Cruzamiento y mutaci\u00f3n\n            new_population = []\n            \n            # Elitismo: mantener los mejores individuos\n            n_elite = max(1, int(self.elitism * self.n_population))\n            elite_indices = np.argsort(fitness_scores)[-n_elite:]\n            new_population.extend([population[i] for i in elite_indices])\n            \n            # Generar nueva poblaci\u00f3n\n            while len(new_population) < self.n_population:\n                # Seleccionar padres aleatorios\n                idx1, idx2 = np.random.choice(len(selected_population), 2, replace=False)\n                parent1, parent2 = selected_population[idx1], selected_population[idx2]\n                \n                # Cruzamiento\n                child1, child2 = self.crossover(parent1, parent2)\n                \n                # Mutaci\u00f3n\n                child1 = self.mutation(child1)\n                child2 = self.mutation(child2)\n                \n                # A\u00f1adir hijos si hay espacio\n                if len(new_population) < self.n_population:\n                    new_population.append(child1)\n                if len(new_population) < self.n_population:\n                    new_population.append(child2)\n            \n            population = new_population[:self.n_population]\n        \n        # Encontrar el mejor individuo de todas las generaciones\n        best_overall_idx = np.argmax(self.best_fitness_history)\n        return best_individual_history[best_overall_idx]\n    \n    def plot_evolution(self):\n        \"\"\"Visualiza el progreso de la evoluci\u00f3n\"\"\"\n        plt.figure(figsize=(10, 6))\n        plt.plot(self.best_fitness_history, label='Mejor Fitness', marker='o')\n        plt.plot(self.avg_fitness_history, label='Fitness Promedio', marker='s')\n        plt.xlabel('Generaci\u00f3n')\n        plt.ylabel('Fitness')\n        plt.title('Progreso de la Neuroevoluci\u00f3n')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig('evolution_progress.png')\n        plt.show()\n\n# Ejecutar neuroevoluci\u00f3n\nif __name__ == \"__main__\":\n    print(\"=== NEUROEVOLUTION CON ALGORITMOS GEN\u00c9TICOS ===\")\n    print(\"Configurando algoritmo gen\u00e9tico...\")\n    \n    # Par\u00e1metros reducidos para evitar errores\n    neuro_evo = GeneticNeuroevolution(n_population=8, n_generations=4)\n    best_architecture = neuro_evo.evolve()\n\n    print(f\"\\n=== MEJOR ARQUITECTURA ENCONTRADA ===\")\n    print(f\"N\u00famero de capas ocultas: {best_architecture[0]}\")\n    print(f\"Unidades por capa: {best_architecture[1:1+best_architecture[0]]}\")\n    print(f\"Dropout rate: {best_architecture[1+best_architecture[0]]:.3f}\")\n    print(f\"Learning rate: {10**best_architecture[2+best_architecture[0]]:.6f}\")\n\n    # Visualizar progreso\n    print(\"\\nGenerando gr\u00e1fica de evoluci\u00f3n...\")\n    neuro_evo.plot_evolution()\n\n    # Entrenar y evaluar el mejor modelo\n    print(\"\\n=== ENTRENANDO MEJOR MODELO ===\")\n    best_model = neuro_evo.build_model(best_architecture)\n    if best_model is not None:\n        history = best_model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), verbose=1)\n        \n        # Evaluar rendimiento final\n        test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n        print(f\"\\nPrecisi\u00f3n final en prueba: {test_accuracy:.4f}\")\n        "]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Ejecutar neuroevoluci\u00f3n (puedes ajustar poblaci\u00f3n y generaciones)\nif __name__ == \"__main__\":\n    neuro_evo = GeneticNeuroevolution(n_population=8, n_generations=4)\n    best_architecture = neuro_evo.evolve()\n\n    print(f\"\\n=== MEJOR ARQUITECTURA ENCONTRADA ===\")\n    print(f\"N\u00famero de capas ocultas: {best_architecture[0]}\")\n    print(f\"Unidades por capa: {best_architecture[1:1+best_architecture[0]]}\")\n    print(f\"Dropout rate: {best_architecture[1+best_architecture[0]]:.3f}\")\n    print(f\"Learning rate: {10**best_architecture[2+best_architecture[0]]:.6f}\")\n\n    neuro_evo.plot_evolution()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}